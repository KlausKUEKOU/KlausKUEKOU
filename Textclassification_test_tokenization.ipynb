{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NgApIzcrbEYWl-8-BqzHVn2HLwMakO_B",
      "authorship_tag": "ABX9TyPHBf0JqtfppTGCMaly/lmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlausKUEKOU/KlausKUEKOU/blob/main/Textclassification_test_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5_1hOCBCQO6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé les ressources nécessaires\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Définir les stop words pour l'anglais\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Gestion des données\n",
        "class DataHandler:\n",
        "    def __init__(self, data_path):\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError(f\"Le fichier {data_path} est introuvable. Veuillez le télécharger.\")\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def load_data(self):\n",
        "        data = pd.read_csv(self.data_path)\n",
        "        data = data[['Text', 'Score']]  # Conserver les colonnes pertinentes\n",
        "        data = data.rename(columns={'Text': 'text', 'Score': 'label'})\n",
        "        data['label'] = data['label'].apply(self.map_score_to_category)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def map_score_to_category(score):\n",
        "        if score <= 2:\n",
        "            return 0  # Insatisfait\n",
        "        elif score == 3:\n",
        "            return 1  # Neutre\n",
        "        else:\n",
        "            return 2  # Satisfait\n",
        "\n",
        "# Charger les données avec DataHandler\n",
        "data_path = \"./Reviews.csv\"\n",
        "data_handler = DataHandler(data_path)\n",
        "data = data_handler.load_data()\n",
        "\n",
        "# Diviser les données\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Modèle PyTorch simple\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Fonction de tokenization\n",
        "\n",
        "def tokenize_words(texts):\n",
        "    return [[word for word in nltk.word_tokenize(text) if word.lower() not in stop_words] for text in texts]\n",
        "\n",
        "def tokenize_subwords(texts, tokenizer):\n",
        "    return [[subword for subword in tokenizer.tokenize(text) if subword.lower() not in stop_words] for text in texts]\n",
        "\n",
        "def tokenize_characters(texts):\n",
        "    return [[char for char in text if char.lower() not in stop_words] for text in texts]\n",
        "\n",
        "def tokenize_sentences(texts):\n",
        "    return [[sentence for sentence in nltk.sent_tokenize(text) if all(word.lower() not in stop_words for word in nltk.word_tokenize(sentence))] for text in texts]\n",
        "\n",
        "def tokenize_spaces(texts):\n",
        "    return [[word for word in text.split() if word.lower() not in stop_words] for text in texts]\n",
        "\n",
        "# Partie 1 : Tokenization\n",
        "def tokenize_and_vectorize(train_texts, test_texts, tokenize_func):\n",
        "    train_tokens = [\" \".join(tokens) for tokens in tokenize_func(train_texts)]\n",
        "    test_tokens = [\" \".join(tokens) for tokens in tokenize_func(test_texts)]\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    train_features = vectorizer.fit_transform(train_tokens)\n",
        "    test_features = vectorizer.transform(test_tokens)\n",
        "\n",
        "    return train_features, test_features, vectorizer\n",
        "\n",
        "# Partie 2 : Conversion en Tensors\n",
        "def convert_to_tensors(train_features, test_features, train_labels, test_labels):\n",
        "    train_features = torch.tensor(train_features.toarray(), dtype=torch.float32)\n",
        "    test_features = torch.tensor(test_features.toarray(), dtype=torch.float32)\n",
        "    train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "    test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "    return train_features, test_features, train_labels, test_labels\n",
        "\n",
        "# Partie 3 : Entraînement du modèle\n",
        "def train_model(train_features, train_labels, input_dim, hidden_dim, output_dim):\n",
        "    train_data = TensorDataset(train_features, train_labels)\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Partie 4 : Évaluation du modèle\n",
        "def evaluate_model(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_features).argmax(dim=1)\n",
        "        accuracy = accuracy_score(test_labels, predictions)\n",
        "        precision = precision_score(test_labels, predictions, average='weighted')\n",
        "        recall = recall_score(test_labels, predictions, average='weighted')\n",
        "        f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "\n",
        "# Test des méthodes de tokenization\n",
        "methods = {\n",
        "    \"Mot par mot\": tokenize_words,\n",
        "    \"Sous-mots (BERT)\": lambda texts: tokenize_subwords(texts, AutoTokenizer.from_pretrained(\"bert-base-uncased\")),\n",
        "    \"Caractères\": tokenize_characters,\n",
        "    \"Phrases\": tokenize_sentences,\n",
        "    \"Espaces\": tokenize_spaces,\n",
        "}\n",
        "\n",
        "# Comparaison des performances\n",
        "results = {}\n",
        "for method, func in methods.items():\n",
        "    print(f\"\\n--- Méthode : {method} ---\")\n",
        "\n",
        "    # Tokenization et vectorisation\n",
        "    train_features, test_features, vectorizer = tokenize_and_vectorize(train_texts, test_texts, func)\n",
        "    print(\"Nombre de caractéristiques vectorisées :\", len(vectorizer.get_feature_names_out()))\n",
        "\n",
        "    # Conversion en Tensors\n",
        "    train_features, test_features, train_labels_tensor, test_labels_tensor = convert_to_tensors(\n",
        "        train_features, test_features, train_labels, test_labels\n",
        "    )\n",
        "\n",
        "    # Entraînement du modèle\n",
        "    input_dim = train_features.shape[1]\n",
        "    hidden_dim = 100\n",
        "    output_dim = len(set(train_labels.values))\n",
        "    model = train_model(train_features, train_labels_tensor, input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # Évaluation\n",
        "    report = evaluate_model(model, test_features, test_labels_tensor)\n",
        "    print(\"Rapport de classification :\", report)\n",
        "\n",
        "    results[method] = report\n",
        "\n",
        "# Sauvegarder les résultats dans un fichier texte\n",
        "with open(\"results.txt\", \"w\") as f:\n",
        "    for method, metrics in results.items():\n",
        "        f.write(f\"\\n--- Méthode : {method} ---\\n\")\n",
        "        for metric, value in metrics.items():\n",
        "            f.write(f\"{metric}: {value:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import os\n"
      ],
      "metadata": {
        "id": "K9c2jxCwCVJV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assurez-vous d'avoir téléchargé les ressources nécessaires\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Définir les stop words pour l'anglais\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2s7iFNXCVuN",
        "outputId": "44942366-fa81-41d2-c4c1-ab71eccd3881"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gestion des données\n",
        "class DataHandler:\n",
        "    def __init__(self, data_path):\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError(f\"Le fichier {data_path} est introuvable. Veuillez le télécharger.\")\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def load_data(self):\n",
        "        data = pd.read_csv(self.data_path)\n",
        "        data = data[['Text', 'Score']]  # Conserver les colonnes pertinentes\n",
        "        data = data.rename(columns={'Text': 'text', 'Score': 'label'})\n",
        "        data['label'] = data['label'].apply(self.map_score_to_category)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def map_score_to_category(score):\n",
        "        if score <= 2:\n",
        "            return 0  # Insatisfait\n",
        "        elif score == 3:\n",
        "            return 1  # Neutre\n",
        "        else:\n",
        "            return 2  # Satisfait"
      ],
      "metadata": {
        "id": "ezYweUBRCWGE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données avec DataHandler\n",
        "data_path = \"/content/drive/MyDrive/Reviews.csv\"\n",
        "data_handler = DataHandler(data_path)\n",
        "data = data_handler.load_data()\n"
      ],
      "metadata": {
        "id": "SLoaYzx1CWYM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diviser les données\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "fWFOi4wACXl8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle PyTorch simple\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "EqADFIkTCXed"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de tokenization\n",
        "\n",
        "def tokenize_words(texts):\n",
        "    return [[word for word in nltk.word_tokenize(text) if word.lower() not in stop_words] for text in texts]\n",
        "\n",
        "def tokenize_subwords(texts, tokenizer):\n",
        "    return [[subword for subword in tokenizer.tokenize(text) if subword.lower() not in stop_words] for text in texts]\n",
        "\n",
        "def tokenize_characters(texts):\n",
        "    return [[char for char in text ] for text in texts]\n",
        "\n",
        "def tokenize_sentences(texts):\n",
        "    return [[sentence for sentence in nltk.sent_tokenize(text) if all(word.lower() not in stop_words for word in nltk.word_tokenize(sentence))] for text in texts]\n",
        "\n",
        "def tokenize_spaces(texts):\n",
        "    return [[word for word in text.split() if word.lower() not in stop_words] for text in texts]\n"
      ],
      "metadata": {
        "id": "cBf4yZiwCXVs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partie 1 : Tokenization\n",
        "def tokenize_and_vectorize(train_texts, test_texts, tokenize_func):\n",
        "    train_tokens = [\" \".join(tokens) for tokens in tokenize_func(train_texts)]\n",
        "    test_tokens = [\" \".join(tokens) for tokens in tokenize_func(test_texts)]\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    train_features = vectorizer.fit_transform(train_tokens)\n",
        "    test_features = vectorizer.transform(test_tokens)\n",
        "\n",
        "    return train_features, test_features, vectorizer"
      ],
      "metadata": {
        "id": "KZZ_zUcHCXMF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partie 2 : Conversion en Tensors\n",
        "def convert_to_tensors(train_features, test_features, train_labels, test_labels):\n",
        "    train_features = torch.tensor(train_features.toarray(), dtype=torch.float32)\n",
        "    test_features = torch.tensor(test_features.toarray(), dtype=torch.float32)\n",
        "    train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "    test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "    return train_features, test_features, train_labels, test_labels\n"
      ],
      "metadata": {
        "id": "iK-OYpXdCXB8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partie 3 : Entraînement du modèle\n",
        "def train_model(train_features, train_labels, input_dim, hidden_dim, output_dim):\n",
        "    train_data = TensorDataset(train_features, train_labels)\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QrDSzyMFCWqU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partie 4 : Évaluation du modèle\n",
        "def evaluate_model(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_features).argmax(dim=1)\n",
        "        accuracy = accuracy_score(test_labels, predictions)\n",
        "        precision = precision_score(test_labels, predictions, average='weighted')\n",
        "        recall = recall_score(test_labels, predictions, average='weighted')\n",
        "        f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KcNqHk8bDNOt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test des méthodes de tokenization\n",
        "methods = {\n",
        "    \"Mot par mot\": tokenize_words,\n",
        "    \"Sous-mots (BERT)\": lambda texts: tokenize_subwords(texts, AutoTokenizer.from_pretrained(\"bert-base-uncased\")),\n",
        "    \"Caractères\": tokenize_characters,\n",
        "    \"Phrases\": tokenize_sentences,\n",
        "    \"Espaces\": tokenize_spaces,\n",
        "}\n"
      ],
      "metadata": {
        "id": "XF2xu6ubDNLt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des performances\n",
        "results = {}\n",
        "for method, func in methods.items():\n",
        "    print(f\"\\n--- Méthode : {method} ---\")\n",
        "\n",
        "    # Tokenization et vectorisation\n",
        "    train_features, test_features, vectorizer = tokenize_and_vectorize(train_texts, test_texts, func)\n",
        "    print(\"Nombre de caractéristiques vectorisées :\", len(vectorizer.get_feature_names_out()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs11dmXdDNI8",
        "outputId": "f55b6da2-240b-44c8-bcbe-471b5e0ebe0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Méthode : Mot par mot ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for method, func in methods.items():\n",
        "    print(f\"\\n--- Méthode : {method} ---\")\n",
        "\n",
        "        # Conversion en Tensors\n",
        "    train_features, test_features, train_labels_tensor, test_labels_tensor = convert_to_tensors(\n",
        "        train_features, test_features, train_labels, test_labels\n",
        "    )\n"
      ],
      "metadata": {
        "id": "UZXYr8Z4J4jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, func in methods.items():\n",
        "    print(f\"\\n--- Méthode : {method} ---\")\n",
        "\n",
        "     # Entraînement du modèle\n",
        "    input_dim = train_features.shape[1]\n",
        "    hidden_dim = 100\n",
        "    output_dim = len(set(train_labels.values))\n",
        "    model = train_model(train_features, train_labels_tensor, input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "nmxKPrl0J5iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, func in methods.items():\n",
        "    print(f\"\\n--- Méthode : {method} ---\")\n",
        "\n",
        "    # Évaluation\n",
        "    report = evaluate_model(model, test_features, test_labels_tensor)\n",
        "    print(\"Rapport de classification :\", report)\n",
        "\n",
        "    results[method] = report"
      ],
      "metadata": {
        "id": "r48SxuDfJ5uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sauvegarder les résultats dans un fichier texte\n",
        "with open(\"results.txt\", \"w\") as f:\n",
        "    for method, metrics in results.items():\n",
        "        f.write(f\"\\n--- Méthode : {method} ---\\n\")\n",
        "        for metric, value in metrics.items():\n",
        "            f.write(f\"{metric}: {value:.4f}\\n\")"
      ],
      "metadata": {
        "id": "jccTsSMuDNFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjAHq1l-DNC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6WsTu7BDM_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXRLWVeRDM8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJAsatYjDMyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DI6PsMWEDMvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02H0oj8VDMVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}